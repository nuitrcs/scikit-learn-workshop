{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0276527-2eee-4dd7-89b6-befeb39e3b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:rebeccapurple\">ML workflow steps:</span>\n",
    "\n",
    "1. State the problem\n",
    "2. Gather the data\n",
    "3. Split train-test sets\n",
    "4. Pre-process the data\n",
    "5. Establish a baseline\n",
    "6. Choose a model\n",
    "7. Train the model\n",
    "8. Optimize the model\n",
    "9. Validate the model\n",
    "10. Predict unknown data points using the model\n",
    "11. Interpret and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e6e11-389a-4edb-9b32-eef018d04f17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:rebeccapurple\">Setup</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95983202-8e62-4789-b1fb-e9b07ebee3ac",
   "metadata": {},
   "source": [
    "**Scikit-learn (Sklearn)** is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f1237-f931-4272-aa29-d667d8a41631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875d4d7-b9e6-4d21-bf5a-53bd8b7985b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some figure display paramaters\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white', {'axes.linewidth': 0.5})\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['xtick.major.size'] = 3\n",
    "plt.rcParams['xtick.major.width'] = 1\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['legend.edgecolor'] = 'w'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51812ab-79c6-4b66-8c8e-c8f081aa8970",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:rebeccapurple\">1. State the problem</span></h1>\n",
    "\n",
    "**Task:** Predict the body mass of penguins.\n",
    "\n",
    "**Input:** Table of penguins features.\n",
    "\n",
    "**Output:** A value for body mass in grams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e7335-9761-4909-abfb-4ab05cfe0b63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:rebeccapurple\">2. Gather and inspect the data</span></h1>\n",
    "\n",
    "**Data:** Size measurements for 344 adult foraging Ad√©lie, Chinstrap, and Gentoo penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adeb1f1-5915-4858-9660-09cf62422eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the data\n",
    "df = pd.read_csv('data/penguins.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899a19d-b2e3-4210-a3ad-26e5dd30c39c",
   "metadata": {},
   "source": [
    "<h4><span style=\"color:blue\">Google Colab users only -- un-comment the code lines below and run them to download the dataset and read it</span></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf3048-981d-48e0-826e-a41f0a280e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/nuitrcs/scikit-learn-workshop/main/data/penguins.csv\n",
    "# df = pd.read_csv('penguins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5461b-d645-432c-a951-657bc8b9f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d23977-cdee-4cef-b523-e680c41c1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb7963-aec6-4fcb-a989-47a0fbe0203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what distribution of the output variable looks like\n",
    "plt.figure(figsize=(4, 2))\n",
    "sns.histplot(df['body_mass_g'], bins=20)\n",
    "plt.title('Target distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1853e8-6611-47f0-8b93-48395fd54a43",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### <span style=\"color:green\">-------- CONCEPT: Feature vs Target split --------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceedb0e-0f28-4f25-ba9d-3969c06bad6a",
   "metadata": {},
   "source": [
    "Once the problem statement is defined, the data can be split into target labels and input features as below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f3379-801c-4466-bf06-ce430c181f40",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><font size=\"+1\">What is the input data for prediction?</font></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2da4a-8abb-4dd8-978c-a3483f585d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the input into the model- features that will be used to predict y\n",
    "X = df.drop(columns=\"body_mass_g\")\n",
    "df.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b4f7ec-2368-4b85-aa29-ce0fe53df35b",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><font size=\"+1\">What is the target to be predicted?</font></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f060af-cfd2-4540-a3b9-e07cd1a4b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the target labels\n",
    "y = df.body_mass_g\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323072b-6ccd-4cbf-94e4-7dd3233afbaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:rebeccapurple\">3. Split the data into train and test</span></h1>\n",
    "Ideally you want to separate the train and test sets very early on. I prefer to split them before pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9e594-9655-469b-96ea-6620150d7281",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:green\">-------- CONCEPT: Train-test split --------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6fe37-d554-481e-846a-44ab9e2600f9",
   "metadata": {},
   "source": [
    "An important aspect of machine learning that sets itself apart from other fields is that in addition to the training error, we want to minimize the generalization error. In other words, we want to make sure that the trained model generalizes well to unobserved/future inputs. That is why we need to split our dataset into train and test set.\n",
    "\n",
    "These 2 sets should be disjoint, i.e., one instance shouldn't be in both sets. (When dataset is too small, there are special measures but we will not cover here.)\n",
    "\n",
    "The typical split is 80%-20% for train vs test set. In sklearn it is defaulted to 75-25 (or more accurately 0.75-0.25).\n",
    "\n",
    "The train dataset is used to learn the model. The test dataset is used to estimate generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a787bc-19d2-4f4c-ba32-baf5bc405231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the function train_test_split from sklearn library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd6d97-3eb3-4022-b9fa-14e7e42edf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape , X_test.shape , y_train.shape , y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2111967-086a-4ef0-a4b5-e772d75f9ae0",
   "metadata": {},
   "source": [
    "Learn more about the train-test split function - https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea1d080-a3ae-4c67-81e9-75a2846963de",
   "metadata": {},
   "source": [
    "<span style=\"color:#DC537D\"><font size=\"+1\">DIY -- Write code to split the dataset 80-20 train vs test</font></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff501482-d548-4bef-9e9c-4c1f4cb7ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "# what is the shape of the new split sets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca957a70-c639-4ce2-8597-9a7793ffe92a",
   "metadata": {},
   "source": [
    "## <span style=\"color:rebeccapurple\">4. Pre-process the training data</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc81d31-2a34-478a-9223-cd3f2520b25e",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">-------- CONCEPT: Data Processing (Fit-Transform) --------</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a2787-a271-426b-930d-74c2991859ea",
   "metadata": {},
   "source": [
    "Raw data can take on any range of values. By preprocessing data, we make it easier to interpret and use.\n",
    "There are many ways to preproccess data for ML, depending on the modeling purpose and data characteristics.\n",
    "\n",
    "We're going to discuss methods to deal with these three types of data:\n",
    "\n",
    "* Numerical\n",
    "* Categorical\n",
    "* Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa612bd1-858c-40bc-a4f2-b71056831a41",
   "metadata": {},
   "source": [
    "<span style=\"color:#DC537D\"><font size=\"+1\">DIY -- What kind of variables does our dataset have? (numeric, categorical, others?)</font></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa7a98-66ff-4e2e-a86e-0bfc28f93090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa5c66-55cf-4dd4-8b4c-7dd161d02eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the numerical features\n",
    "numeric_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c90ac7-1403-492f-a303-34652bd392bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the categorical features\n",
    "categorical_cols = ['species', 'island', 'sex', 'year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e014d-25b6-42b1-8b86-bd992873838d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#1409FA\">Scaling Numerical data</span>\n",
    "Several machine learning algorithms rely on calculating distances. Therefore, it is important to have all the input features on the same scale - so that the distances computed for different features are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b3d9ca-4310-42d9-848b-a51b2f0829a1",
   "metadata": {},
   "source": [
    "#### <span style=\"color:teal\">Standardization</span>\n",
    "This is the process of converting data into the standard format where each feature has zero mean and unit variance (i.e., std=1).\n",
    "$$ x' = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00931b4b-b318-460c-a5ec-74f9ed358ff1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T23:19:38.375498Z",
     "iopub.status.busy": "2024-04-21T23:19:38.374538Z",
     "iopub.status.idle": "2024-04-21T23:19:38.808514Z",
     "shell.execute_reply": "2024-04-21T23:19:38.808227Z",
     "shell.execute_reply.started": "2024-04-21T23:19:38.375425Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17bef5-c6e0-461c-8789-827f06ed9f45",
   "metadata": {},
   "source": [
    "The StandardScaler object will find the fit parameters $\\mu$ and $\\sigma$ for each feature $x$ in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbce36f-9468-45e0-a5fc-571722190668",
   "metadata": {},
   "source": [
    "#### <span style=\"color:teal\">Scaling features to a range</span>\n",
    "Apart from standardization, we can scale features to lie between a given minimum and maximum value, often between zero and one. Range compression helps with robustness due to small standard deviations of features after scaling and at the same time preserves zero entries.\n",
    "$$ x' = \\frac{x - min}{max - min} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a42d2d-faaf-4e83-95aa-270916373ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59725006-b031-4425-bf5f-81efd74d17be",
   "metadata": {},
   "source": [
    "The MinMaxScaler object will find the fit parameters $min$ and $max$ for each feature $x$ in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd09a4-ebfa-4718-bb80-ab83c24f87c9",
   "metadata": {},
   "source": [
    "#### <span style=\"color:teal\">Normalization</span>\n",
    "This is the process of scaling individual samples to have unit norm. So far, we have scaled data by features i.e., calculations are applied on individual columns. Sometimes, we need to scale data across rows. For example, some clustering techniques require normalization to calculate cosine similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199347c-f907-4a5e-82cf-066a536dd7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee4ef5-0a9f-4a47-99fc-4157b8bebb11",
   "metadata": {},
   "source": [
    "### <span style=\"color:rebeccapurple\">4.1 Fit-transform numerical features with Standardization</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f06bef-efe6-46f4-9014-c8435386eca3",
   "metadata": {},
   "source": [
    "Let's suppose we choose to do Standardization on the numerical features. This is how we would go about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313345f-8f8a-4915-b09d-8124f67f1145",
   "metadata": {},
   "source": [
    "### I. Find the fit parameters for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac772a6-d4b6-4c10-9a23-5715b745eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class \"StandardScaler\" from the scikit learn library\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a26b1-b1e0-4d55-a577-c0b1a58ec0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler is an object or instance of class StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84091d85-9d87-4e92-b0f7-16c1fa509963",
   "metadata": {},
   "source": [
    "Hover on the \"i\" icon - what does it say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b31030-c7e0-44f2-ae63-b0f5f6aaa71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now feed your data for pre-processing into this object\n",
    "scaler.fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7c115-17fd-43f5-8d9d-0c40a086a0bd",
   "metadata": {},
   "source": [
    "What happened here? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579df95-3fd6-4169-a046-36ca1542a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the numeric columns\n",
    "scaler.fit(X_train[numeric_cols])\n",
    "scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f8f8c-b995-4d60-b842-b2917042f269",
   "metadata": {},
   "source": [
    "Hover on the \"i\" icon - what does it say now? <br>\n",
    "Once the data is fit, it means that the scaler has calculated the parameters for scaling i.e. the mean and the standard deviation, for each of the numeric columns. However, we still need to calculate the scaled (also called transformed) values (x prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73092d",
   "metadata": {},
   "source": [
    "$$ x' = \\frac{x - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb2deff-89b2-45f2-8303-2da93e852886",
   "metadata": {},
   "source": [
    "### II. Transform the train AND test data with the fit paramaters you found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219baa0-823e-45ac-803f-d9f745d7094f",
   "metadata": {},
   "source": [
    "Important note: preprocessing should be fit to train dataset, instead of applying to the whole set, to avoid leakage of information. Data leakage essentially means that the training process has information about, and thus will create a bias toward, the test set, potentially leading to a deceptively good generalization result.\n",
    "\n",
    "After the preprocessor learns to fit the train dataset, it is then used to transform the train and test set. More specifically, in the case of standardization,the preprocessor will obtain the mean and standard deviation of the train set and use those statistics to transform both the train and test set. As a result, after the transformation, the train set will definitely have 0 mean and unit standard deviation but the test set may not. Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19310c-bbd7-4afb-a2dd-dc86d76f1d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the transformed X_train and X_test data\n",
    "numeric_X_train = scaler.transform(X_train[numeric_cols])\n",
    "numeric_X_test  = scaler.transform(X_test[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8931385-6471-47db-a5a4-65f5cf8c8451",
   "metadata": {},
   "source": [
    "Let's see what the transformed data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9466a83-195c-438f-96c1-97e10c73478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_X_train_df = pd.DataFrame(data = numeric_X_train, columns= numeric_cols)\n",
    "display(numeric_X_train_df.head(2))\n",
    "display(X_train[numeric_cols].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cc07e-3ef8-4b31-81e1-8a5c8e078c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_X_train_df.describe().loc[\"mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb0fef-f1d7-45f2-8f0d-5abe22b56f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-21T22:50:39.822671Z",
     "iopub.status.busy": "2024-04-21T22:50:39.822040Z",
     "iopub.status.idle": "2024-04-21T22:50:39.831642Z",
     "shell.execute_reply": "2024-04-21T22:50:39.830643Z",
     "shell.execute_reply.started": "2024-04-21T22:50:39.822636Z"
    }
   },
   "source": [
    "What do you notice about the mean of the transformed values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc431e4-44a5-4838-b77a-2927f49f71d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_X_train_df.describe().loc[\"std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b593988-0601-47ce-b5ff-8f0930f5c4ea",
   "metadata": {},
   "source": [
    "What do you notice about the standard deviation of the transformed values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06d62bd-55bb-4bbb-b289-8727da4616e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#1409FA\">Encoding Categorical data</span>\n",
    "Data sometimes come in non-numeric values in predictors and/or response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f98d0-59b7-4bca-ab93-9d39897f1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e2d86-6f39-4709-85d8-5af43ae0e8f5",
   "metadata": {},
   "source": [
    "#### <span style=\"color:teal\">Ordinal encoding</span>\n",
    "This is the process of assigning each unique category an integer value. Doing this, we impose a natural ordered relationship between each category.\n",
    "\n",
    "For example, age is ordered in nature and we can map the different ranges to integer values. More specifically, 30-39 => 0, 40-49 =>1, 50-59 => 2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0f157-fe9a-4a4a-886a-98494ed6a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ae901-3164-44f7-b49b-204319789252",
   "metadata": {},
   "source": [
    "#### <span style=\"color:teal\">One-hot encoding</span>\n",
    "When there is no natural ordinal relationship among different categories, OrdinalEncoder is not an appropriate approach.\n",
    "\n",
    "In addition, when the response variable has no ordinal relationship, encoding its labels as ordered integer values can result in poor performance. For example, suppose we encode the response's labels as 0, 1, 2. The algorithm can return a prediction of 1.5.\n",
    "\n",
    "One-hot encoding is the process of transforming each label of the orginal categorical variable into a new binary variable. This means the total number of features will increase after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516d436-a05e-4a57-99c4-d465060375fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d6795-6cb6-4864-81f6-fbbe7afcc908",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### <span style=\"color:teal\">Label encoding</span>\n",
    "sklearn has a separate module to encode the target variable itself - this would be used if the target is categorical. For example, if we were to predict the sex of the penguin based on the other features then the l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709f4cf-9b8c-48c7-9128-e412ed9594fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f66675-0bc1-4bb3-9ad2-8e9bc17ad39e",
   "metadata": {},
   "source": [
    "We will learn how to encode the categorical features of our dataset in the next lesson when we perform a linear regression!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
